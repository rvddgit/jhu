{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing Code Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary libraries for this demonstration. Note that you may need to install libraries if you have not used them before.  If you have already loaded these libraries skip the pip install step and move straight to importing the necessary libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[K     |████████████████████████████████| 626 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk>=3.8\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 29.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Collecting click\n",
      "  Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, click, nltk, textblob\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 textblob-0.18.0.post0 tqdm-4.67.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/ianmcculloh/myenvGL/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting googletrans==4.0.0-rc1\n",
      "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
      "Collecting httpx==0.13.3\n",
      "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 3.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet==3.*\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
      "Collecting rfc3986<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting idna==2.*\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 21.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpcore==0.9.*\n",
      "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
      "Collecting hstspreload\n",
      "  Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 17.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11<0.10,>=0.8\n",
      "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 6.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting h2==3.*\n",
      "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 18.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting hpack<4,>=3.0\n",
      "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
      "Collecting hyperframe<6,>=5.2.0\n",
      "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
      "Using legacy 'setup.py install' for googletrans, since package 'wheel' is not installed.\n",
      "Installing collected packages: hyperframe, hpack, h2, h11, rfc3986, idna, httpcore, hstspreload, chardet, httpx, googletrans\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.14.0\n",
      "    Uninstalling h11-0.14.0:\n",
      "      Successfully uninstalled h11-0.14.0\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.10\n",
      "    Uninstalling idna-3.10:\n",
      "      Successfully uninstalled idna-3.10\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.6\n",
      "    Uninstalling httpcore-1.0.6:\n",
      "      Successfully uninstalled httpcore-1.0.6\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.27.2\n",
      "    Uninstalling httpx-0.27.2:\n",
      "      Successfully uninstalled httpx-0.27.2\n",
      "    Running setup.py install for googletrans ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab 4.2.5 requires httpx>=0.25.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\n",
      "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/ianmcculloh/myenvGL/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.1 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "\u001b[K     |████████████████████████████████| 383 kB 43.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.6 MB 23.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.24.0\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 41.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 36.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ianmcculloh/myenvGL/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Installing collected packages: fsspec, filelock, huggingface-hub, tokenizers, safetensors, transformers\n",
      "Successfully installed filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.0 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.47.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/ianmcculloh/myenvGL/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install googletrans==4.0.0-rc1  # Latest compatible version\n",
    "!pip install transformers\n",
    "!pip install spacy\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianmcculloh/myenvGL/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from textblob import TextBlob\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from googletrans import Translator\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a toy dataset for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example dataset\n",
    "data = {\n",
    "    \"tweet\": [\n",
    "        \"I love the new iPhone! It's fantastic. #Apple\",\n",
    "        \"The service at this restaurant was terrible. Never going back. #Disappointed\",\n",
    "        \"Tesla's new model is groundbreaking! #Innovation\",\n",
    "        \"I had an average experience with the product. It's okay. #Neutral\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert dataset to DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text Classification:\n",
      "                                               tweet  classification\n",
      "0      I love the new iPhone! It's fantastic. #Apple               1\n",
      "1  The service at this restaurant was terrible. N...               0\n",
      "2   Tesla's new model is groundbreaking! #Innovation               1\n",
      "3  I had an average experience with the product. ...               1\n"
     ]
    }
   ],
   "source": [
    "# 1. Text Classification (Binary sentiment classification: positive/negative)\n",
    "def text_classification():\n",
    "    # Label tweets manually for demonstration\n",
    "    df[\"label\"] = [1, 0, 1, 1]  # 1 = positive, 0 = negative\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df[\"tweet\"])\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X, df[\"label\"])\n",
    "    predictions = model.predict(X)\n",
    "    df[\"classification\"] = predictions\n",
    "    print(\"\\nText Classification:\")\n",
    "    print(df[[\"tweet\", \"classification\"]])\n",
    "\n",
    "text_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentiment Analysis (Polarity Scores):\n",
      "                                               tweet  sentiment\n",
      "0      I love the new iPhone! It's fantastic. #Apple   0.356818\n",
      "1  The service at this restaurant was terrible. N...  -0.583333\n",
      "2   Tesla's new model is groundbreaking! #Innovation   0.170455\n",
      "3  I had an average experience with the product. ...   0.175000\n"
     ]
    }
   ],
   "source": [
    "# 2. Sentiment Analysis\n",
    "def sentiment_analysis():\n",
    "    df[\"sentiment\"] = df[\"tweet\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    print(\"\\nSentiment Analysis (Polarity Scores):\")\n",
    "    print(df[[\"tweet\", \"sentiment\"]])\n",
    "\n",
    "sentiment_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Named Entity Recognition (NER):\n",
      "                                               tweet             entities\n",
      "0      I love the new iPhone! It's fantastic. #Apple    [(Apple, PERSON)]\n",
      "1  The service at this restaurant was terrible. N...                   []\n",
      "2   Tesla's new model is groundbreaking! #Innovation       [(Tesla, ORG)]\n",
      "3  I had an average experience with the product. ...  [(Neutral, PERSON)]\n"
     ]
    }
   ],
   "source": [
    "# 3. Named Entity Recognition (NER)\n",
    "def named_entity_recognition():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    df[\"entities\"] = df[\"tweet\"].apply(lambda x: [(ent.text, ent.label_) for ent in nlp(x).ents])\n",
    "    print(\"\\nNamed Entity Recognition (NER):\")\n",
    "    print(df[[\"tweet\", \"entities\"]])\n",
    "\n",
    "named_entity_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of Speech (POS) tagging function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parts of Speech (POS) Tagging:\n",
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                            pos_tags  \n",
      "0  [(I, PRON), (love, VERB), (the, DET), (new, AD...  \n",
      "1  [(The, DET), (service, NOUN), (at, ADP), (this...  \n",
      "2  [(Tesla, PROPN), ('s, PART), (new, ADJ), (mode...  \n",
      "3  [(I, PRON), (had, VERB), (an, DET), (average, ...  \n"
     ]
    }
   ],
   "source": [
    "# 4. Parts of Speech (POS) Tagging\n",
    "def pos_tagging():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    df[\"pos_tags\"] = df[\"tweet\"].apply(lambda x: [(token.text, token.pos_) for token in nlp(x)])\n",
    "    print(\"\\nParts of Speech (POS) Tagging:\")\n",
    "    print(df[[\"tweet\", \"pos_tags\"]])\n",
    "\n",
    "pos_tagging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine translation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Machine Translation (English to Spanish):\n",
      "                                               tweet  \\\n",
      "0      I love the new iPhone! It's fantastic. #Apple   \n",
      "1  The service at this restaurant was terrible. N...   \n",
      "2   Tesla's new model is groundbreaking! #Innovation   \n",
      "3  I had an average experience with the product. ...   \n",
      "\n",
      "                                          translated  \n",
      "0        ¡Amo el nuevo iPhone!Es fantástico.#Manzana  \n",
      "1  El servicio en este restaurante fue terrible.N...  \n",
      "2  ¡El nuevo modelo de Tesla es innovador!#Innova...  \n",
      "3  Tuve una experiencia promedio con el producto....  \n"
     ]
    }
   ],
   "source": [
    "# 5. Machine Translation\n",
    "def machine_translation():\n",
    "    translator = Translator()\n",
    "    df[\"translated\"] = df[\"tweet\"].apply(lambda x: translator.translate(x, src=\"en\", dest=\"es\").text)\n",
    "    print(\"\\nMachine Translation (English to Spanish):\")\n",
    "    print(df[[\"tweet\", \"translated\"]])\n",
    "\n",
    "machine_translation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvGL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
